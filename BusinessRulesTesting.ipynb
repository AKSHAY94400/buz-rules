{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     22,
     239,
     290,
     309,
     348,
     371
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Ashyam Zubair\n",
    "Created Date: 14-02-2019\n",
    "\"\"\"\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import sqlalchemy\n",
    "import os\n",
    "\n",
    "from MySQLdb._exceptions import OperationalError\n",
    "from sqlalchemy import create_engine, exc\n",
    "from time import time\n",
    "\n",
    "# try:\n",
    "#     from app.ace_logger import Logging\n",
    "# except:\n",
    "#     from ace_logger import Logging\n",
    "     \n",
    "# logging = Logging()\n",
    "\n",
    "import logging\n",
    "class DB(object):\n",
    "    def __init__(self, database, host='127.0.0.1', user='root', password='', port='3306', tenant_id=None):\n",
    "        \"\"\"\n",
    "        Initialization of databse object.\n",
    "\n",
    "        Args:\n",
    "            databse (str): The database to connect to.\n",
    "            host (str): Host IP address. For dockerized app, it is the name of\n",
    "                the service set in the compose file.\n",
    "            user (str): Username of MySQL server. (default = 'root')\n",
    "            password (str): Password of MySQL server. For dockerized app, the\n",
    "                password is set in the compose file. (default = '')\n",
    "            port (str): Port number for MySQL. For dockerized app, the port that\n",
    "                is mapped in the compose file. (default = '3306')\n",
    "        \"\"\"\n",
    "\n",
    "        if host in [\"common_db\",\"extraction_db\", \"queue_db\", \"template_db\", \"table_db\", \"stats_db\", \"business_rules_db\", \"reports_db\"]:\n",
    "            self.HOST = os.environ['HOST_IP']\n",
    "            self.USER = 'root'\n",
    "            self.PASSWORD = os.environ['LOCAL_DB_PASSWORD']\n",
    "            self.PORT = '3306'\n",
    "            self.DATABASE = f'{tenant_id}_{database}' if tenant_id is not None and tenant_id else database\n",
    "        else:\n",
    "            self.HOST = host\n",
    "            self.USER = user\n",
    "            self.PASSWORD = password\n",
    "            self.PORT = port\n",
    "            self.DATABASE = f'{tenant_id}_{database}' if tenant_id is not None and tenant_id else database\n",
    "        \n",
    "        logging.info(f'Host: {self.HOST}')\n",
    "        logging.info(f'User: {self.USER}')\n",
    "        logging.info(f'Password: {self.PASSWORD}')\n",
    "        logging.info(f'Port: {self.PORT}')\n",
    "        logging.info(f'Database: {self.DATABASE}')\n",
    "\n",
    "        self.connect()\n",
    "\n",
    "    def connect(self, max_retry=5):\n",
    "        retry = 1\n",
    "\n",
    "        try:\n",
    "            start = time()\n",
    "            logging.debug(f'Making connection to `{self.DATABASE}`...')\n",
    "            config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{self.DATABASE}?charset=utf8'\n",
    "            self.db_ = create_engine(config, connect_args={'connect_timeout': 2}, pool_recycle=300)\n",
    "            logging.info(f'Engine created for `{self.DATABASE}`')\n",
    "            while retry <= max_retry:\n",
    "                try:\n",
    "                    self.engine = self.db_.connect()\n",
    "                    logging.info(f'Connection established succesfully to `{self.DATABASE}`! ({round(time() - start, 2)} secs to connect)')\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    logging.warning(f'Connection failed. Retrying... ({retry}) [{e}]')\n",
    "                    retry += 1\n",
    "                    self.db_.dispose()\n",
    "        except:\n",
    "            logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "            return\n",
    "\n",
    "    def execute(self, query, database=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Executes an SQL query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query that needs to be executed.\n",
    "            database (str): Name of the database to execute the query in. Leave\n",
    "                it none if you want use database during object creation.\n",
    "            params (list/tuple/dict): List of parameters to pass to in the query.\n",
    "\n",
    "        Returns:\n",
    "            (DataFrame) A pandas dataframe containing the data from the executed\n",
    "            query. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        data = None\n",
    "\n",
    "        # Use new database if a new databse is given\n",
    "        if database is not None:\n",
    "            try:\n",
    "                config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "                engine = create_engine(config, pool_recycle=300)\n",
    "            except:\n",
    "                logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "                return False\n",
    "        else:\n",
    "            engine = self.engine\n",
    "\n",
    "        try:\n",
    "            logging.debug(f'Query: {query}')\n",
    "            data = pd.read_sql(query, engine, index_col='id', **kwargs)\n",
    "        except exc.ResourceClosedError:\n",
    "            logging.warning('Query does not have any value to return.')\n",
    "            return True\n",
    "        except (exc.StatementError, OperationalError) as e:\n",
    "            logging.warning(f'Creating new connection. Engine/Connection is probably None. [{e}]')\n",
    "            self.connect()\n",
    "            data = pd.read_sql(query, self.engine, index_col='id', **kwargs)\n",
    "        except:\n",
    "            logging.exception('Something went wrong executing query. Check trace.')\n",
    "            params = kwargs['params'] if 'params' in kwargs else None\n",
    "            return False\n",
    "\n",
    "        return data.where((pd.notnull(data)), None)\n",
    "\n",
    "    def execute_(self, query, database=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Executes an SQL query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query that needs to be executed.\n",
    "            database (str): Name of the database to execute the query in. Leave\n",
    "                it none if you want use database during object creation.\n",
    "            params (list/tuple/dict): List of parameters to pass to in the query.\n",
    "\n",
    "        Returns:\n",
    "            (DataFrame) A pandas dataframe containing the data from the executed\n",
    "            query. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        data = None\n",
    "\n",
    "        # Use new database if a new databse is given\n",
    "        if database is not None:\n",
    "            try:\n",
    "                config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "                engine = create_engine(config, pool_recycle=300)\n",
    "            except:\n",
    "                logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "                return False\n",
    "        else:\n",
    "            engine = self.engine\n",
    "\n",
    "        try:\n",
    "            data = pd.read_sql(query, engine, **kwargs)\n",
    "        except exc.ResourceClosedError:\n",
    "            return True\n",
    "        except:\n",
    "            logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "            params = kwargs['params'] if 'params' in kwargs else None\n",
    "            return False\n",
    "\n",
    "        return data.replace({pd.np.nan: None})\n",
    "\n",
    "\n",
    "    def insert(self, data, table, database=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Write records stored in a DataFrame to a SQL database.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The DataFrame that needs to be write to SQL database.\n",
    "            table (str): The table in which the rcords should be written to.\n",
    "            database (str): The database the table lies in. Leave it none if you\n",
    "                want use database during object creation.\n",
    "            kwargs: Keyword arguments for pandas to_sql function.\n",
    "                See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html\n",
    "                to know the arguments that can be passed.\n",
    "\n",
    "        Returns:\n",
    "            (bool) True is succesfully inserted, else false.\n",
    "        \"\"\"\n",
    "        logging.info(f'Inserting into `{table}`')\n",
    "\n",
    "        # Use new database if a new databse is given\n",
    "        if database is not None:\n",
    "            try:\n",
    "                config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "                engine = create_engine(config, pool_recycle=300)\n",
    "            except:\n",
    "                logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "                return False\n",
    "        else:\n",
    "            engine = self.engine\n",
    "\n",
    "        try:\n",
    "            data.to_sql(table, engine, **kwargs)\n",
    "            try:\n",
    "                self.execute(f'ALTER TABLE `{table}` ADD PRIMARY KEY (`id`);')\n",
    "            except:\n",
    "                pass\n",
    "            return True\n",
    "        except:\n",
    "            logging.exception('Something went wrong inserting. Check trace.')\n",
    "            return False\n",
    "\n",
    "    def insert_dict(self, data, table):\n",
    "        \"\"\"\n",
    "        Insert dictionary into a SQL database table.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The DataFrame that needs to be write to SQL database.\n",
    "            table (str): The table in which the rcords should be written to.\n",
    "\n",
    "        Returns:\n",
    "            (bool) True is succesfully inserted, else false.\n",
    "        \"\"\"\n",
    "        logging.info(f'Inserting dictionary data into `{table}`...')\n",
    "        logging.debug(f'Data:\\n{data}')\n",
    "\n",
    "        try:\n",
    "            column_names = []\n",
    "            params = []\n",
    "\n",
    "            for column_name, value in data.items():\n",
    "                column_names.append(f'`{column_name}`')\n",
    "                params.append(value)\n",
    "\n",
    "            logging.debug(f'Column names: {column_names}')\n",
    "            logging.debug(f'Params: {params}')\n",
    "\n",
    "            columns_string = ', '.join(column_names)\n",
    "            param_placeholders = ', '.join(['%s'] * len(column_names))\n",
    "\n",
    "            query = f'INSERT INTO {table} ({columns_string}) VALUES ({param_placeholders})'\n",
    "\n",
    "            return self.execute(query, params=params)\n",
    "        except:\n",
    "            logging.exception('Error inserting data.')\n",
    "            return False\n",
    "\n",
    "    def update(self, table, update=None, where=None, database=None, force_update=False):\n",
    "        # Use new database if a new databse is given\n",
    "        if database is not None:\n",
    "            try:\n",
    "                config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "                self.engine = create_engine(config, pool_recycle=300)\n",
    "            except:\n",
    "                logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "                return False\n",
    "\n",
    "        logging.info(f'Updating table: `{table}`')\n",
    "        logging.info(f'Update data: `{update}`')\n",
    "        logging.info(f'Where clause data: `{where}`')\n",
    "        logging.info(f'Force update flag: `{force_update}`')\n",
    "\n",
    "        try:\n",
    "            set_clause = []\n",
    "            set_value_list = []\n",
    "            where_clause = []\n",
    "            where_value_list = []\n",
    "\n",
    "            if where is not None and where:\n",
    "                for set_column, set_value in update.items():\n",
    "                    set_clause.append(f'`{set_column}`=%s')\n",
    "                    set_value_list.append(set_value)\n",
    "                set_clause_string = ', '.join(set_clause)\n",
    "            else:\n",
    "                logging.error(f'Update dictionary is None/empty. Must have some update clause.')\n",
    "                return False\n",
    "\n",
    "            if where is not None and where:\n",
    "                for where_column, where_value in where.items():\n",
    "                    where_clause.append(f'{where_column}=%s')\n",
    "                    where_value_list.append(where_value)\n",
    "                where_clause_string = ' AND '.join(where_clause)\n",
    "                query = f'UPDATE `{table}` SET {set_clause_string} WHERE {where_clause_string}'\n",
    "            else:\n",
    "                if force_update:\n",
    "                    query = f'UPDATE `{table}` SET {set_clause_string}'\n",
    "                else:\n",
    "                    message = 'Where dictionary is None/empty. If you want to force update every row, pass force_update as True.'\n",
    "                    logging.error(message)\n",
    "                    return False\n",
    "\n",
    "            params = set_value_list + where_value_list\n",
    "            self.execute(query, params=params)\n",
    "            return True\n",
    "        except:\n",
    "            logging.exception('Something went wrong updating. Check trace.')\n",
    "            return False\n",
    "\n",
    "    def get_column_names(self, table, database=None):\n",
    "        \"\"\"\n",
    "        Get all column names from an SQL table.\n",
    "\n",
    "        Args:\n",
    "            table (str): Name of the table from which column names should be extracted.\n",
    "            database (str): Name of the database in which the table lies. Leave\n",
    "                it none if you want use database during object creation.\n",
    "\n",
    "        Returns:\n",
    "            (list) List of headers. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(f'Getting column names of table `{table}`')\n",
    "            return list(self.execute(f'SELECT * FROM `{table}`', database))\n",
    "        except:\n",
    "            logging.exception('Something went wrong getting column names. Check trace.')\n",
    "            return\n",
    "\n",
    "    def execute_default_index(self, query, database=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Executes an SQL query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query that needs to be executed.\n",
    "            database (str): Name of the database to execute the query in. Leave\n",
    "                it none if you want use database during object creation.\n",
    "            params (list/tuple/dict): List of parameters to pass to in the query.\n",
    "\n",
    "        Returns:\n",
    "            (DataFrame) A pandas dataframe containing the data from the executed\n",
    "            query. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        data = None\n",
    "\n",
    "        # Use new database if a new databse is given\n",
    "        if database is not None:\n",
    "            try:\n",
    "                config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "                engine = create_engine(config, pool_recycle=300)\n",
    "            except:\n",
    "                logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "                return False\n",
    "        else:\n",
    "            engine = self.engine\n",
    "\n",
    "        try:\n",
    "            data = pd.read_sql(query, engine, **kwargs)\n",
    "        except exc.ResourceClosedError:\n",
    "            return True\n",
    "        except:\n",
    "            logging.exception(f'Something went wrong while executing query. Check trace.')\n",
    "            params = kwargs['params'] if 'params' in kwargs else None\n",
    "            return False\n",
    "\n",
    "        return data.where((pd.notnull(data)), None)\n",
    "\n",
    "\n",
    "    def get_all(self, table, database=None, discard=None):\n",
    "        \"\"\"\n",
    "        Get all data from an SQL table.\n",
    "\n",
    "        Args:\n",
    "            table (str): Name of the table from which data should be extracted.\n",
    "            database (str): Name of the database in which the table lies. Leave\n",
    "                it none if you want use database during object creation.\n",
    "            discard (list): columns to be excluded while selecting all\n",
    "        Returns:\n",
    "            (DataFrame) A pandas dataframe containing the data. (None if an error\n",
    "            occurs)\n",
    "        \"\"\"\n",
    "        logging.info(f'Getting all data from `{table}`')\n",
    "        if discard:\n",
    "            logging.info(f'Discarding columns `{discard}`')\n",
    "            columns = list(self.execute_default_index(f'SHOW COLUMNS FROM `{table}`',database).Field)\n",
    "            columns = [col for col in columns if col not in discard]\n",
    "            columns_str = json.dumps(columns).replace(\"'\",'`').replace('\"','`')[1:-1]\n",
    "            return self.execute(f'SELECT {columns_str} FROM `{table}`', database)\n",
    "\n",
    "        return self.execute(f'SELECT * FROM `{table}`', database)\n",
    "\n",
    "    def get_latest(self, data, group_by_col, sort_col):\n",
    "        \"\"\"\n",
    "        Group data by a column containing repeated values and get latest from it by\n",
    "        taking the latest value based on another column.\n",
    "\n",
    "        Example:\n",
    "        Get the latest products\n",
    "            id     product   date\n",
    "            220    6647     2014-09-01\n",
    "            220    6647     2014-10-16\n",
    "            826    3380     2014-11-11\n",
    "            826    3380     2015-05-19\n",
    "            901    4555     2014-09-01\n",
    "            901    4555     2014-11-01\n",
    "\n",
    "        The function will return\n",
    "            id     product   date\n",
    "            220    6647     2014-10-16\n",
    "            826    3380     2015-05-19\n",
    "            901    4555     2014-11-01\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): Pandas DataFrame to query on.\n",
    "            group_by_col (str): Column containing repeated values.\n",
    "            sort_col (str): Column to identify the latest record.\n",
    "\n",
    "        Returns:\n",
    "            (DataFrame) Contains the latest records. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info('Grouping data...')\n",
    "            logging.info(f'Data: {data}')\n",
    "            logging.info(f'Group by column: {group_by_col}')\n",
    "            logging.info(f'Sort column: {sort_col}')\n",
    "            return data.sort_values(sort_col).groupby(group_by_col).tail(1)\n",
    "        except KeyError as e:\n",
    "            logging.errot(f'Column `{e.args[0]}` does not exist.')\n",
    "            return None\n",
    "        except:\n",
    "            logging.exception('Something went wrong while grouping data.')\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HOST_IP'] = '35.173.139.208'\n",
    "os.environ['LOCAL_DB_USER'] = 'root'\n",
    "os.environ['LOCAL_DB_PASSWORD'] = 'AlgoTeam123'\n",
    "os.environ['LOCAL_DB_PORT'] = '3306'\n",
    "\n",
    "tenant_id = 'deloitte.acelive.ai'\n",
    "\n",
    "db_config = {\n",
    "    'host': os.environ['HOST_IP'],\n",
    "    'user': os.environ['LOCAL_DB_USER'],\n",
    "    'password': os.environ['LOCAL_DB_PASSWORD'],\n",
    "    'port': os.environ['LOCAL_DB_PORT']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     354,
     356,
     364
    ]
   },
   "outputs": [],
   "source": [
    "%load apply_business_rule.py\n",
    "try:\n",
    "    from ace_logger import Logging\n",
    "    logging = Logging()\n",
    "except:\n",
    "    import logging \n",
    "    logger=logging.getLogger() \n",
    "    logger.setLevel(logging.DEBUG) \n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "from db_utils import DB \n",
    "\n",
    "try:\n",
    "    from BusinessRules import BusinessRules\n",
    "except:\n",
    "    from .BusinessRules import BusinessRules\n",
    "\n",
    "\n",
    "# one configuration\n",
    "db_config = {\n",
    "    'host': os.environ['HOST_IP'],\n",
    "    'user': os.environ['LOCAL_DB_USER'],\n",
    "    'password': os.environ['LOCAL_DB_PASSWORD'],\n",
    "    'port': os.environ['LOCAL_DB_PORT']\n",
    "}\n",
    "\n",
    "def to_DT_data(parameters):\n",
    "    \"\"\"Amith's processing for parameters\"\"\"\n",
    "    output = []\n",
    "    try:\n",
    "        for param_dict in parameters:\n",
    "            print(param_dict)    \n",
    "            if param_dict['column'] == 'Add_on_Table':\n",
    "                output.append({'table': param_dict['table'],'column': param_dict['column'],'value': param_dict['value']})\n",
    "                # Need to add a function to show this or tell Kamal check if its addon table and parse accordingly\n",
    "            else:                \n",
    "                output.append({'table': param_dict['table'],'column': param_dict['column'],'value': param_dict['value']})\n",
    "    except:\n",
    "        print(\"Error in to_DT_data()\")\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "    try:\n",
    "        output = [dict(t) for t in {tuple(d.items()) for d in output}]\n",
    "    except:\n",
    "        print(\"Error in removing duplicate dictionaries in list\")\n",
    "        traceback.print_exc()\n",
    "        pass\n",
    "    return output\n",
    "\n",
    "def get_data_sources(tenant_id, case_id, column_name, master=False):\n",
    "    \"\"\"Helper to get all the required table data for the businesss rules to apply\n",
    "    \"\"\"\n",
    "    get_datasources_query = \"SELECT * from `data_sources`\"\n",
    "    db_config['tenant_id'] = tenant_id\n",
    "    business_rules_db = DB('business_rules', **db_config)\n",
    "    data_sources = business_rules_db.execute(get_datasources_query)\n",
    "\n",
    "\n",
    "    # sources\n",
    "    sources = json.loads(list(data_sources[column_name])[0])\n",
    "    \n",
    "    \n",
    "    data = {}\n",
    "    for database, tables in sources.items():\n",
    "        db = DB(database, **db_config)\n",
    "        for table in tables:\n",
    "            if master:\n",
    "                query = f\"SELECT * from `{table}`\"\n",
    "                try:\n",
    "                    df = db.execute(query)\n",
    "                except:\n",
    "                    df = db.execute_(query)\n",
    "                    \n",
    "                data[table] = df.to_dict(orient='records')\n",
    "            else:\n",
    "                query = f\"SELECT * from `{table}` WHERE case_id = %s\"\n",
    "                params = [case_id]\n",
    "                df = db.execute(query, params=params)\n",
    "                if not df.empty:\n",
    "                    data[table] = df.to_dict(orient='records')[0]\n",
    "                else:\n",
    "                    data[table] = {}\n",
    "    \n",
    "    \n",
    "    case_id_based_sources = json.loads(list(data_sources['case_id_based'])[0])\n",
    "    \n",
    "    return data\n",
    "                \n",
    "def get_rules(tenant_id, group):\n",
    "    \"\"\"Get the rules based on the stage, tenant_id\"\"\"\n",
    "    db_config['tenant_id'] = tenant_id\n",
    "    business_rules_db = DB('business_rules', **db_config)\n",
    "    get_rules_query = \"SELECT * from `sequence_rule_data` where `group` = %s\"\n",
    "    params = [group]\n",
    "    rules = business_rules_db.execute(get_rules_query, params=params)\n",
    "    return rules\n",
    "\n",
    "def update_tables(case_id, tenant_id, updates):\n",
    "    \"\"\"Update the values in the database\"\"\"\n",
    "    db_config['tenant_id'] = tenant_id\n",
    "    extraction_db = DB('extraction', **db_config) # only in ocr or process_queue we are updating\n",
    "    queue_db = DB('queues', **db_config) # only in ocr or process_queue we are updating\n",
    "    \n",
    "    for table, colum_values in updates.items():\n",
    "        if table == 'ocr':\n",
    "            extraction_db.update(table, update=colum_values, where={'case_id':case_id})\n",
    "        if table == 'process_queue':\n",
    "            queue_db.update(table, update=colum_values, where={'case_id':case_id})\n",
    "    return \"UPDATED IN THE DATABASE SUCCESSFULLY\"\n",
    "\n",
    "def run_chained_rules(case_id, tenant_id, chain_rules, start_rule_id=None, updated_tables=False, trace_exec=None, rule_params=None):\n",
    "    \"\"\"Execute the chained rules\"\"\"\n",
    "    \n",
    "    # get the mapping of the rules...basically a rule_id maps to a rule\n",
    "    rule_id_mapping = {}\n",
    "    for ind, rule in chain_rules.iterrows():\n",
    "        rule_id_mapping[rule['rule_id']] = [rule['rule_string'], rule['next_if_sucess'], rule['next_if_failure'], rule['stage'], rule['description'], rule['data_source']]\n",
    "    logging.info(f\"\\n rule id mapping is \\n{rule_id_mapping}\\n\")\n",
    "    \n",
    "    # evaluate the rules one by one as chained\n",
    "    # start_rule_id = None\n",
    "    if start_rule_id is None:\n",
    "        if rule_id_mapping.keys():\n",
    "            start_rule_id = list(rule_id_mapping.keys())[0]\n",
    "            trace_exec = []\n",
    "            rule_params = {}\n",
    "            \n",
    "    # if start_rule_id then. coming from other service \n",
    "    # get the existing trace and rule params data\n",
    "    db_config['tenant_id'] = tenant_id\n",
    "    business_rules_db = DB('business_rules', **db_config)\n",
    "    rule_data_query = \"SELECT * from `rule_data` where `case_id`=%s\"\n",
    "    params = [case_id]\n",
    "    df = business_rules_db.execute(rule_data_query, params=params)\n",
    "    try:\n",
    "        trace_exec = json.loads(list(df['trace_data'])[0])\n",
    "        logging.info(f\"\\nexistig trace exec is \\n{trace_exec}\\n\")\n",
    "    except Exception as e:\n",
    "        logging.info(f\"no existing trace data\")\n",
    "        logging.info(f\"{str(e)}\")\n",
    "        trace_exec = []\n",
    "    \n",
    "    try:\n",
    "        rule_params = json.loads(list(df['rule_params'])[0])\n",
    "        logging.info(f\"\\nexistig rule_params is \\n{rule_params}\\n\")\n",
    "    except Exception as e:\n",
    "        logging.info(f\"no existing rule params data\")\n",
    "        logging.info(f\"{str(e)}\")\n",
    "        rule_params = {}\n",
    "      \n",
    "    # usually master data will not get updated...for every rule\n",
    "    master_data_tables = get_data_sources(tenant_id, case_id, 'master', master=True)\n",
    " \n",
    "    logging.info(f\"\\nStart rule id got is {start_rule_id}\\n \")\n",
    "    while start_rule_id != \"END\":\n",
    "        # get the rules, next rule id to be evaluated\n",
    "        rule_to_evaluate, next_if_sucess, next_if_failure, stage, description, data_source = rule_id_mapping[str(start_rule_id)]  \n",
    "    \n",
    "        logging.info(f\"\\nInside the loop \\n rule_to_evaluate  {rule_to_evaluate}\\n \\\n",
    "                      \\nnext_if_sucess {next_if_sucess}\\n \\\n",
    "                      \\nnext_if_failure {next_if_failure}\\n \")\n",
    "        \n",
    "        # update the data_table if there is any change\n",
    "        case_id_data_tables = get_data_sources(tenant_id, case_id, 'case_id_based')\n",
    "        master_updated_tables = {} \n",
    "        if updated_tables:\n",
    "            master_updated_tables = get_data_sources(tenant_id, case_id, 'updated_tables')\n",
    "        # consolidate the data into data_tables\n",
    "        data_tables = {**case_id_data_tables, **master_data_tables, **master_updated_tables} \n",
    "        \n",
    "        # evaluate the rule\n",
    "        rules = [json.loads(rule_to_evaluate)] \n",
    "        BR  = BusinessRules(case_id, rules, data_tables)\n",
    "        BR.tenant_id = tenant_id\n",
    "        decision = BR.evaluate_rule(rules[0])\n",
    "        \n",
    "        logging.info(f\"\\n got the decision {decision} for the rule id {start_rule_id}\")\n",
    "        logging.info(f\"\\n updates got are {BR.changed_fields}\")\n",
    "        \n",
    "        updates = {}\n",
    "        # update the updates if any\n",
    "        if BR.changed_fields:\n",
    "            updates = BR.changed_fields\n",
    "            update_tables(case_id, tenant_id, updates)\n",
    "\n",
    "        \n",
    "        # update the trace_data\n",
    "        trace_exec.append(start_rule_id)\n",
    "\n",
    "        logging.info(f\"\\n params data used from the rules are \\n {BR.params_data}\\n\")\n",
    "        # update the rule_params\n",
    "        trace_dict = {\n",
    "                        str(start_rule_id):{\n",
    "                            'description' : description if description else 'No description available in the database',\n",
    "                            'output' : \"\",\n",
    "                            'input' : to_DT_data(BR.params_data['input'])\n",
    "                            }\n",
    "                        }\n",
    "        rule_params.update(trace_dict)\n",
    "        # update the start_rule_id based on the decision\n",
    "        if decision:\n",
    "            start_rule_id = next_if_sucess\n",
    "        else:\n",
    "            start_rule_id = next_if_failure\n",
    "        logging.info(f\"\\n next rule id to execute is {start_rule_id}\\n\")\n",
    "        \n",
    "    \n",
    "    # off by one updates...\n",
    "    trace_exec.append(start_rule_id)\n",
    "    \n",
    "    # store the trace_exec and rule_params in the database\n",
    "    update_rule_params_query = f\"INSERT INTO `rule_data`(`id`, `case_id`, `rule_params`) VALUES ('NULL',%s,%s) ON DUPLICATE KEY UPDATE `rule_params`=%s\"\n",
    "    params = [case_id, json.dumps(rule_params,default=str), json.dumps(rule_params,default=str)]\n",
    "    business_rules_db.execute(update_rule_params_query, params=params)\n",
    "    \n",
    "    update_trace_exec_query = f\"INSERT INTO `rule_data` (`id`, `case_id`, `trace_data`) VALUES ('NULL',%s,%s) ON DUPLICATE KEY UPDATE `trace_data`=%s\"\n",
    "    params = [case_id, json.dumps(trace_exec), json.dumps(trace_exec)]\n",
    "    business_rules_db.execute(update_trace_exec_query, params=params)\n",
    "    \n",
    "    logging.info(\"\\n Applied chained rules successfully\")\n",
    "    return 'Applied chained rules successfully'\n",
    "\n",
    "def run_group_rules(case_id,tenant_id, rules, data):\n",
    "    \"\"\"Run the rules\"\"\"\n",
    "    rules = [json.loads(rule) for rule in rules] \n",
    "    BR  = BusinessRules(case_id, rules, data)\n",
    "    BR.tenant_id = tenant_id\n",
    "    updates = BR.evaluate_business_rules()\n",
    "    \n",
    "    logging.info(f\"\\n updates from the group rules are \\n{updates}\\n\")\n",
    "    return updates\n",
    "\n",
    "def apply_business_rule(case_id, function_params, tenant_id, start_rule_id=None):\n",
    "    \"\"\"Run the business rules based on the stage in function params and tenant_id\n",
    "    Args:\n",
    "        case_id: Unique id that we pass\n",
    "        function_params: Parameters that we get from the configurations\n",
    "        tenant_id: Tenant on which we have to apply the rules\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        stage = function_params['stage'][0]\n",
    "        logging.debug(f'Running business rules after {stage}')\n",
    "    except:\n",
    "        pass\n",
    "    updates = {} # keep a track of updates that are being made by business rules\n",
    "    try:\n",
    "        # get the stage from the function_parameters...As of now its first ele..\n",
    "        # need to make generic or key-value pairs\n",
    "        logging.info(f\"\\n case_id {case_id} \\nfunction_params {function_params} \\ntenant_id {tenant_id}\\n\")\n",
    "              \n",
    "        # get the rules\n",
    "        rules = get_rules(tenant_id, stage)\n",
    "        \n",
    "        # get the mapping of the rules...basically a rule_id maps to a rule.\n",
    "        # useful for the chain rule evaluations\n",
    "        rule_id_mapping = {}\n",
    "        for ind, rule in rules.iterrows():\n",
    "            rule_id_mapping[rule['rule_id']] = [rule['rule_string'], rule['next_if_sucess'], rule['next_if_failure'], rule['stage'], rule['description'], rule['data_source']]\n",
    "\n",
    "        # making it generic takes to take a type parameter from the database..\n",
    "        # As of now make it (all others  or chained) only\n",
    "        is_chain_rule = '' not in rule_id_mapping\n",
    "        \n",
    "        # get the required table data on which we will be applying business_rules  \n",
    "        case_id_data_tables = get_data_sources(tenant_id, case_id, 'case_id_based') \n",
    "        master_data_tables = get_data_sources(tenant_id, case_id, 'master', master=True)\n",
    "        \n",
    "        # consolidate the data into data_tables\n",
    "        data_tables = {**case_id_data_tables, **master_data_tables} \n",
    "        \n",
    "        logging.info(f\"\\ndata got from the tables is\\n\")\n",
    "        logging.info(data_tables)\n",
    "        \n",
    "        updates = {}\n",
    "        # apply business rules\n",
    "        if is_chain_rule:\n",
    "            run_chained_rules(case_id, tenant_id, rules, start_rule_id)\n",
    "        else:\n",
    "            updates = run_group_rules(case_id, tenant_id, list(rules['rule_string']), data_tables)\n",
    "            \n",
    "        # update in the database, the changed fields eventually when all the stage rules were got\n",
    "        update_tables(case_id, tenant_id, updates)\n",
    "        \n",
    "        #  return the updates for viewing\n",
    "        return {'flag': True, 'message': 'Applied business rules successfully.', 'updates':updates}\n",
    "    except Exception as e:\n",
    "        logging.exception('Something went wrong while applying business rules. Check trace.')\n",
    "        return {'flag': False, 'message': 'Something went wrong while applying business rules. Check logs.', 'error':str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Running business rules after extract\n",
      "INFO:root:\n",
      " case_id DEL7B6DCE0 \n",
      "function_params {'stage': ['extract']} \n",
      "tenant_id deloitte.acelive.ai\n",
      "\n",
      "INFO:root:Host: 35.173.139.208\n",
      "INFO:root:User: root\n",
      "INFO:root:Password: AlgoTeam123\n",
      "INFO:root:Port: 3306\n",
      "INFO:root:Database: deloitte.acelive.ai_business_rules\n",
      "DEBUG:root:Making connection to `deloitte.acelive.ai_business_rules`...\n",
      "INFO:root:Engine created for `deloitte.acelive.ai_business_rules`\n",
      "INFO:root:Connection established succesfully to `deloitte.acelive.ai_business_rules`! (4.3 secs to connect)\n",
      "DEBUG:root:Query: SELECT * from `sequence_rule_data` where `group` = %s\n",
      "INFO:root:Host: 35.173.139.208\n",
      "INFO:root:User: root\n",
      "INFO:root:Password: AlgoTeam123\n",
      "INFO:root:Port: 3306\n",
      "INFO:root:Database: deloitte.acelive.ai_business_rules\n",
      "DEBUG:root:Making connection to `deloitte.acelive.ai_business_rules`...\n",
      "INFO:root:Engine created for `deloitte.acelive.ai_business_rules`\n",
      "INFO:root:Connection established succesfully to `deloitte.acelive.ai_business_rules`! (3.77 secs to connect)\n",
      "DEBUG:root:Query: SELECT * from `data_sources`\n",
      "INFO:root:Host: 35.173.139.208\n",
      "INFO:root:User: root\n",
      "INFO:root:Password: AlgoTeam123\n",
      "INFO:root:Port: 3306\n",
      "INFO:root:Database: deloitte.acelive.ai_extraction\n",
      "DEBUG:root:Making connection to `deloitte.acelive.ai_extraction`...\n",
      "INFO:root:Engine created for `deloitte.acelive.ai_extraction`\n",
      "INFO:root:Connection established succesfully to `deloitte.acelive.ai_extraction`! (4.1 secs to connect)\n",
      "DEBUG:root:Query: SELECT * from `ocr` WHERE case_id = %s\n",
      "INFO:root:Host: 35.173.139.208\n",
      "INFO:root:User: root\n",
      "INFO:root:Password: AlgoTeam123\n",
      "INFO:root:Port: 3306\n",
      "INFO:root:Database: deloitte.acelive.ai_queues\n",
      "DEBUG:root:Making connection to `deloitte.acelive.ai_queues`...\n",
      "INFO:root:Engine created for `deloitte.acelive.ai_queues`\n",
      "INFO:root:Connection established succesfully to `deloitte.acelive.ai_queues`! (3.95 secs to connect)\n",
      "DEBUG:root:Query: SELECT * from `process_queue` WHERE case_id = %s\n",
      "INFO:root:Host: 35.173.139.208\n",
      "INFO:root:User: root\n",
      "INFO:root:Password: AlgoTeam123\n",
      "INFO:root:Port: 3306\n",
      "INFO:root:Database: deloitte.acelive.ai_business_rules\n",
      "DEBUG:root:Making connection to `deloitte.acelive.ai_business_rules`...\n",
      "INFO:root:Engine created for `deloitte.acelive.ai_business_rules`\n",
      "INFO:root:Connection established succesfully to `deloitte.acelive.ai_business_rules`! (3.64 secs to connect)\n",
      "DEBUG:root:Query: SELECT * from `data_sources`\n",
      "INFO:root:Host: 35.173.139.208\n",
      "INFO:root:User: root\n",
      "INFO:root:Password: AlgoTeam123\n",
      "INFO:root:Port: 3306\n",
      "INFO:root:Database: deloitte.acelive.ai_extraction\n",
      "DEBUG:root:Making connection to `deloitte.acelive.ai_extraction`...\n",
      "INFO:root:Engine created for `deloitte.acelive.ai_extraction`\n",
      "INFO:root:Connection established succesfully to `deloitte.acelive.ai_extraction`! (4.08 secs to connect)\n",
      "DEBUG:root:Query: SELECT * from `vendor_master`\n",
      "DEBUG:root:Query: SELECT * from `client_master`\n",
      "DEBUG:root:Query: SELECT * from `gst_compliance`\n",
      "DEBUG:root:Query: SELECT * from `hsn_master`\n",
      "DEBUG:root:Query: SELECT * from `ltd_master`\n",
      "DEBUG:root:Query: SELECT * from `pincode_gst_validation`\n",
      "DEBUG:root:Query: SELECT * from `po_master`\n",
      "DEBUG:root:Query: SELECT * from `rcm_master`\n",
      "DEBUG:root:Query: SELECT * from `tds_master`\n",
      "INFO:root:\n",
      "data got from the tables is\n",
      "\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "INFO:root:\n",
      " rule id mapping is \n",
      "{'48': ['{\\r\\n  \"rule_type\": \"condition\",\\r\\n  \"evaluations\": [\\r\\n    {\\r\\n      \"conditions\": [\\r\\n        {\\r\\n          \"rule_type\": \"static\",\\r\\n          \"function\": \"AmountCompare\",\\r\\n          \"parameters\": {\\r\\n            \"left_param\": {\\r\\n              \"source\": \"input_config\",\\r\\n              \"table\": \"ocr\",\\r\\n              \"column\": \"total_value\"\\r\\n            },\\r\\n            \"operator\": \">=\",\\r\\n            \"right_param\": {\\r\\n              \"source\": \"input\",\\r\\n              \"value\": \"1000000\"\\r\\n            }\\r\\n          }\\r\\n        }\\r\\n      ],\\r\\n      \"executions\": [\\r\\n        {\\r\\n          \"rule_type\": \"static\",\\r\\n          \"function\": \"Assign\",\\r\\n          \"parameters\": {\\r\\n            \"assign_table\": {\\r\\n              \"table\": \"process_queue\",\\r\\n              \"column\": \"queue\"\\r\\n            },\\r\\n            \"assign_value\": {\\r\\n              \"source\": \"input\",\\r\\n              \"value\": \"validation\"\\r\\n            }\\r\\n          }\\r\\n        }\\r\\n      ]\\r\\n    }\\r\\n  ]\\r\\n}', 'END', 'END', 'extract', 'Amount_100000 compare', '']}\n",
      "\n",
      "INFO:root:Host: 35.173.139.208\n",
      "INFO:root:User: root\n",
      "INFO:root:Password: AlgoTeam123\n",
      "INFO:root:Port: 3306\n",
      "INFO:root:Database: deloitte.acelive.ai_business_rules\n",
      "DEBUG:root:Making connection to `deloitte.acelive.ai_business_rules`...\n",
      "INFO:root:Engine created for `deloitte.acelive.ai_business_rules`\n",
      "INFO:root:Connection established succesfully to `deloitte.acelive.ai_business_rules`! (3.67 secs to connect)\n",
      "DEBUG:root:Query: SELECT * from `rule_data` where `case_id`=%s\n",
      "INFO:root:\n",
      "existig trace exec is \n",
      "['1', '3', '4', 'END', '1', '3', '5', '6', '4', 'END', '1', '3', '5', '6', '4', 'END', '1', '3', '5', '6', '4', 'END', '7', 'END', '7', 'END', '1', '3', '5', '6', '8', '10', '11', '13', '14', '4', 'END', '7', 'END', '7', 'END', '7', 'END', '1', '3', '5', '6', '8', '10', '11', '13', '14', '15', '16', '17', '4', 'END', '1', '3', '5', '6', '7', '8', '10', '11', '13', '14', '15', '16', '17', '4', 'END', '1', '3', '5', '6', '7', '8', '10', '11', '13', '14', '15', '16', '17', '19', '20', '4', 'END', '1', '3', '5', '6', '7', '8', '10', '11', '13', '14', '15', '16', '17', '19', '20', '22', '24', '4', 'END', '1', '3', '4', 'END', '1', '3', '5', '6', '7', '8', '10', '11', '13', '14', '15', '16', '17', '4', 'END', '1', '3', '5', '6', '7', '8', '10', '11', '13', '14', '15', '16', '17', '4', 'END', '1', '3', '5', '6', '7', '8', '10', '11', '13', '14', '15', '16', '17', '4', 'END', '1', '3', '5', '6', '7', '8', '10', '11', '13', '14', '15', '16', '17', '19', '20', '4', 'END', '1', '3', '5', '6', '7', '8', '10', '11', '13', '14', '15', '16', '17', '19', '20', '22', '24', '4', 'END', '1', '3', '5', '6', '7', '8', '10', '4', 'END', '1', '3', '5', '6', '7', '8', '10', '11', '13', '14', '15', '16', '17', '19', '20', '22', '24', '4', 'END', '1', '3', '5', '6', '7', '8', '10', '11', '13', '14', '15', '16', '17', '19', '20', '22', '24', '25', '26', '28', '4', 'END', '1', '3', '49', '50', '48', '5', '6', '7', '8', '10', '11', '13', '14', '15', '16', '17', '19', '20', '4', 'END', '1', '3', '49', '50', '48', '5', '6', '7', '8', '10', '11', '13', '14', '15', '16', '17', '19', '20', '4', 'END', '1', '3', '49', '50', '48', '5', '6', '7', '8', '10', '11', '13', '14', '15', '16', '17', '19', '20', '4', 'END', '48', 'END', '48', 'END', '48', 'END', '48', 'END', '1', '3', '49', '50', '5', '6', '7', '8', '10', '11', '13', '14', '15', '16', '17', '19', '20', '4', 'END']\n",
      "\n",
      "INFO:root:\n",
      "existig rule_params is \n",
      "{'1': {'description': 'checking `proforma Invoice` is present in invoice_header', 'output': '', 'input': [{'table': 'ocr', 'column': 'invoice_header', 'value': 'TAX INVOICE'}]}, '3': {'description': 'checking \"Debit note\" or \"credit note\" or \"invoice\" or \"Tax invoice\" is present in invoice_header', 'output': '', 'input': [{'table': 'ocr', 'column': 'invoice_header', 'value': 'TAX INVOICE'}]}, '4': {'description': 'Moving case to validation queue.', 'output': '', 'input': []}, '5': {'description': 'Date Conversion to a dd-mm-yyyy format for invoice_date', 'output': '', 'input': [{'table': 'ocr', 'column': 'invoice_date', 'value': ''}]}, '6': {'description': 'Checking Country == `India` is Resident or Not', 'output': '', 'input': [{'table': 'ocr', 'column': 'vendor_name', 'value': 'Nielsen (India) Private Limited'}]}, '7': {'description': 'Checking Client_name is Present or not in endor_master', 'output': '', 'input': [{'table': 'ocr', 'column': 'client_name', 'value': 'Facebook India Online Services Pvt Ltd'}]}, '8': {'description': 'Checking Vendor_PAN is Empty', 'output': '', 'input': [{'table': 'ocr', 'column': 'vendor_pan', 'value': 'AAACM9279L'}]}, '10': {'description': 'Comparing OCR Vendor PAN with Vendor_Master PAN', 'output': '', 'input': [{'table': 'ocr', 'column': 'vendor_name', 'value': 'Nielsen (India) Private Limited'}, {'table': 'ocr', 'column': 'vendor_pan', 'value': 'AAACM9279L'}]}, '11': {'description': 'Checking Client_PAN is Empty', 'output': '', 'input': [{'table': 'ocr', 'column': 'client_pan', 'value': 'AABCF5150G'}]}, '13': {'description': 'Comparing OCR Client PAN with Client_Master PAN', 'output': '', 'input': [{'table': 'ocr', 'column': 'client_pan', 'value': 'AABCF5150G'}, {'table': 'ocr', 'column': 'client_name', 'value': 'Facebook India Online Services Pvt Ltd'}]}, '14': {'description': 'Validating The Vendor_PAN according to government syntax', 'output': '', 'input': [{'table': 'ocr', 'column': 'vendor_pan', 'value': 'AAACM9279L'}]}, '15': {'description': 'Validating The Client_PAN according to government syntax', 'output': '', 'input': [{'table': 'ocr', 'column': 'client_pan', 'value': 'AABCF5150G'}]}, '16': {'description': 'Checking Vendor_pincode == \"\"', 'output': '', 'input': [{'table': 'ocr', 'column': 'address_of_vendor', 'value': 'Nielsen (India) Private Limited Godrej IT Park, C Block, 6th Floor, Godrej Business Dist,Phirojshanagar,LBS Marg,Vikhroli(W) Mumbai 400079 India PAN No.: AAACM9279L E-Mail Id : India.Accountreceivable®nielsen.com'}]}, '17': {'description': 'Checking Vendor_GSTIN == GST_Code based on pincode', 'output': '', 'input': [{'table': 'ocr', 'column': 'address_of_vendor', 'value': 'Nielsen (India) Private Limited Godrej IT Park, C Block, 6th Floor, Godrej Business Dist,Phirojshanagar,LBS Marg,Vikhroli(W) Mumbai 400079 India PAN No.: AAACM9279L E-Mail Id : India.Accountreceivable®nielsen.com'}, {'table': 'ocr', 'column': 'vendor_gstin', 'value': '27AAACM9279L1ZS'}]}, '19': {'description': 'Checking Client_pincode == \"\"', 'output': '', 'input': [{'table': 'ocr', 'column': 'address_of_client', 'value': 'Navdeep Kumar Facebook India Online Services Pvt. Ltd. DNE BKC, 7th Floor A Wing,Unit No 703, C-32 3 Block, Bandra Kurla Complex,Bandra (E), Mumbai 400051 Maharashtra, India'}]}, '20': {'description': 'Checking Client_GSTIN == GST_Code based on Client_pincode', 'output': '', 'input': [{'table': 'ocr', 'column': 'address_of_client', 'value': 'Navdeep Kumar Facebook India Online Services Pvt. Ltd. DNE BKC, 7th Floor A Wing,Unit No 703, C-32 3 Block, Bandra Kurla Complex,Bandra (E), Mumbai 400051 Maharashtra, India'}, {'table': 'ocr', 'column': 'client_gstin', 'value': '36AABCF5150G1ZW'}]}, '22': {'description': 'Checking both OCR_VendorGSTIN and Vendor_masterGSTIN are empty', 'output': '', 'input': [{'table': 'ocr', 'column': 'vendor_gstin', 'value': '27AAACM9279L1ZS'}, {'table': 'ocr', 'column': 'vendor_name', 'value': 'Nielsen (India) Private Limited'}]}, '24': {'description': 'Checking OCR.Vendor_GSTIN == Vendor_Master.GSTIN', 'output': '', 'input': [{'table': 'ocr', 'column': 'vendor_gstin', 'value': '27AAACM9279L1ZS'}, {'table': 'ocr', 'column': 'vendor_name', 'value': 'Nielsen (India) Private Limited'}]}, '25': {'description': 'Checking Syntax of OCR.Vendor_GSTIN ', 'output': '', 'input': [{'table': 'ocr', 'column': 'vendor_pan', 'value': 'AAACM9279L'}, {'table': 'ocr', 'column': 'vendor_gstin', 'value': '27AAACM9279L1ZS'}, {'table': 'ocr', 'column': 'address_of_vendor', 'value': 'Nielsen (India) Private Limited Godrej IT Park, C Block, 6th Floor, Godrej Business Dist,Phirojshanagar,LBS Marg,Vikhroli(W) Mumbai 400079 India PAN No.: AAACM9279L E-Mail Id : India.Accountreceivable®nielsen.com'}]}, '26': {'description': 'Checking both OCR_ClientGSTIN and Client_masterGSTIN are empty', 'output': '', 'input': [{'table': 'ocr', 'column': 'client_gstin', 'value': '27AABCF5150G1ZW'}, {'table': 'ocr', 'column': 'client_name', 'value': 'Facebook India Online Services Pvt Ltd'}]}, '28': {'description': 'Checking OCR.Client_GSTIN == Client_Master.GSTIN', 'output': '', 'input': [{'table': 'ocr', 'column': 'client_gstin', 'value': '27AABCF5150G1ZW'}, {'table': 'ocr', 'column': 'client_name', 'value': 'Facebook India Online Services Pvt Ltd'}]}, '49': {'description': 'Checking resident or non_resident', 'output': '', 'input': [{'table': 'ocr', 'column': 'vendor_name', 'value': 'Nielsen (India) Private Limited'}]}, '50': {'description': 'Assigning client_name from client master based partial_match', 'output': '', 'input': [{'table': 'ocr', 'column': 'client_name', 'value': 'Facebook India Online Services Pvt Ltd'}]}, '48': {'description': 'Amount_100000 compare', 'output': '', 'input': [{'table': 'ocr', 'column': 'total_value', 'value': '1,260,594.00'}]}}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Host: 35.173.139.208\n",
      "INFO:root:User: root\n",
      "INFO:root:Password: AlgoTeam123\n",
      "INFO:root:Port: 3306\n",
      "INFO:root:Database: deloitte.acelive.ai_business_rules\n",
      "DEBUG:root:Making connection to `deloitte.acelive.ai_business_rules`...\n",
      "INFO:root:Engine created for `deloitte.acelive.ai_business_rules`\n",
      "INFO:root:Connection established succesfully to `deloitte.acelive.ai_business_rules`! (3.74 secs to connect)\n",
      "DEBUG:root:Query: SELECT * from `data_sources`\n",
      "INFO:root:Host: 35.173.139.208\n",
      "INFO:root:User: root\n",
      "INFO:root:Password: AlgoTeam123\n",
      "INFO:root:Port: 3306\n",
      "INFO:root:Database: deloitte.acelive.ai_extraction\n",
      "DEBUG:root:Making connection to `deloitte.acelive.ai_extraction`...\n",
      "INFO:root:Engine created for `deloitte.acelive.ai_extraction`\n",
      "INFO:root:Connection established succesfully to `deloitte.acelive.ai_extraction`! (3.97 secs to connect)\n",
      "DEBUG:root:Query: SELECT * from `vendor_master`\n",
      "DEBUG:root:Query: SELECT * from `client_master`\n",
      "DEBUG:root:Query: SELECT * from `gst_compliance`\n",
      "DEBUG:root:Query: SELECT * from `hsn_master`\n",
      "DEBUG:root:Query: SELECT * from `ltd_master`\n",
      "DEBUG:root:Query: SELECT * from `pincode_gst_validation`\n",
      "DEBUG:root:Query: SELECT * from `po_master`\n",
      "DEBUG:root:Query: SELECT * from `rcm_master`\n",
      "DEBUG:root:Query: SELECT * from `tds_master`\n",
      "INFO:root:\n",
      "Start rule id got is 48\n",
      " \n",
      "INFO:root:\n",
      "Inside the loop \n",
      " rule_to_evaluate  {\n",
      "  \"rule_type\": \"condition\",\n",
      "  \"evaluations\": [\n",
      "    {\n",
      "      \"conditions\": [\n",
      "        {\n",
      "          \"rule_type\": \"static\",\n",
      "          \"function\": \"AmountCompare\",\n",
      "          \"parameters\": {\n",
      "            \"left_param\": {\n",
      "              \"source\": \"input_config\",\n",
      "              \"table\": \"ocr\",\n",
      "              \"column\": \"total_value\"\n",
      "            },\n",
      "            \"operator\": \">=\",\n",
      "            \"right_param\": {\n",
      "              \"source\": \"input\",\n",
      "              \"value\": \"1000000\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"executions\": [\n",
      "        {\n",
      "          \"rule_type\": \"static\",\n",
      "          \"function\": \"Assign\",\n",
      "          \"parameters\": {\n",
      "            \"assign_table\": {\n",
      "              \"table\": \"process_queue\",\n",
      "              \"column\": \"queue\"\n",
      "            },\n",
      "            \"assign_value\": {\n",
      "              \"source\": \"input\",\n",
      "              \"value\": \"validation\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "                       \n",
      "next_if_sucess END\n",
      "                       \n",
      "next_if_failure END\n",
      " \n",
      "INFO:root:Host: 35.173.139.208\n",
      "INFO:root:User: root\n",
      "INFO:root:Password: AlgoTeam123\n",
      "INFO:root:Port: 3306\n",
      "INFO:root:Database: deloitte.acelive.ai_business_rules\n",
      "DEBUG:root:Making connection to `deloitte.acelive.ai_business_rules`...\n",
      "INFO:root:Engine created for `deloitte.acelive.ai_business_rules`\n",
      "INFO:root:Connection established succesfully to `deloitte.acelive.ai_business_rules`! (3.93 secs to connect)\n",
      "DEBUG:root:Query: SELECT * from `data_sources`\n",
      "INFO:root:Host: 35.173.139.208\n",
      "INFO:root:User: root\n",
      "INFO:root:Password: AlgoTeam123\n",
      "INFO:root:Port: 3306\n",
      "INFO:root:Database: deloitte.acelive.ai_extraction\n",
      "DEBUG:root:Making connection to `deloitte.acelive.ai_extraction`...\n",
      "INFO:root:Engine created for `deloitte.acelive.ai_extraction`\n",
      "INFO:root:Connection established succesfully to `deloitte.acelive.ai_extraction`! (3.85 secs to connect)\n",
      "DEBUG:root:Query: SELECT * from `ocr` WHERE case_id = %s\n",
      "INFO:root:Host: 35.173.139.208\n",
      "INFO:root:User: root\n",
      "INFO:root:Password: AlgoTeam123\n",
      "INFO:root:Port: 3306\n",
      "INFO:root:Database: deloitte.acelive.ai_queues\n",
      "DEBUG:root:Making connection to `deloitte.acelive.ai_queues`...\n",
      "INFO:root:Engine created for `deloitte.acelive.ai_queues`\n",
      "INFO:root:Connection established succesfully to `deloitte.acelive.ai_queues`! (4.04 secs to connect)\n",
      "DEBUG:root:Query: SELECT * from `process_queue` WHERE case_id = %s\n",
      "INFO:root:\n",
      "Evaluating the rule \n",
      "{'rule_type': 'condition', 'evaluations': [{'conditions': [{'rule_type': 'static', 'function': 'AmountCompare', 'parameters': {'left_param': {'source': 'input_config', 'table': 'ocr', 'column': 'total_value'}, 'operator': '>=', 'right_param': {'source': 'input', 'value': '1000000'}}}], 'executions': [{'rule_type': 'static', 'function': 'Assign', 'parameters': {'assign_table': {'table': 'process_queue', 'column': 'queue'}, 'assign_value': {'source': 'input', 'value': 'validation'}}}]}]}\n",
      "\n",
      "INFO:root:\n",
      "conditions got are \n",
      "[{'rule_type': 'static', 'function': 'AmountCompare', 'parameters': {'left_param': {'source': 'input_config', 'table': 'ocr', 'column': 'total_value'}, 'operator': '>=', 'right_param': {'source': 'input', 'value': '1000000'}}}]\n",
      "\n",
      "INFO:root:\n",
      "executions got are \n",
      "[{'rule_type': 'static', 'function': 'Assign', 'parameters': {'assign_table': {'table': 'process_queue', 'column': 'queue'}, 'assign_value': {'source': 'input', 'value': 'validation'}}}]\n",
      "\n",
      "INFO:root:Evaluting the condition {'rule_type': 'static', 'function': 'AmountCompare', 'parameters': {'left_param': {'source': 'input_config', 'table': 'ocr', 'column': 'total_value'}, 'operator': '>=', 'right_param': {'source': 'input', 'value': '1000000'}}}\n",
      "INFO:root:\n",
      "Evaluating the rule \n",
      "{'rule_type': 'static', 'function': 'AmountCompare', 'parameters': {'left_param': {'source': 'input_config', 'table': 'ocr', 'column': 'total_value'}, 'operator': '>=', 'right_param': {'source': 'input', 'value': '1000000'}}}\n",
      "\n",
      "INFO:root:\n",
      "PARAM OBJECT IS {'source': 'input_config', 'table': 'ocr', 'column': 'total_value'}\n",
      "\n",
      "DEBUG:root:\n",
      "table is ocr and column key is total_value\n",
      "\n",
      "INFO:root:\n",
      "PARAM OBJECT IS {'source': 'input', 'value': '1000000'}\n",
      "\n",
      "DEBUG:root:left param value is 1,260,594.00 and type is <class 'str'>\n",
      "DEBUG:root:right param value is 1000000 and type is <class 'str'>\n",
      "DEBUG:root:operator is >=\n",
      "INFO:root:\n",
      " eval string is  True  \n",
      " output is True\n",
      "INFO:root:\n",
      "Evaluating the rule \n",
      "{'rule_type': 'static', 'function': 'Assign', 'parameters': {'assign_table': {'table': 'process_queue', 'column': 'queue'}, 'assign_value': {'source': 'input', 'value': 'validation'}}}\n",
      "\n",
      "DEBUG:root:parameters got are {'assign_table': {'table': 'process_queue', 'column': 'queue'}, 'assign_value': {'source': 'input', 'value': 'validation'}}\n",
      "INFO:root:\n",
      "PARAM OBJECT IS {'source': 'input', 'value': 'validation'}\n",
      "\n",
      "INFO:root:Updated the data source with the values process_queue queue\n",
      " \n",
      "INFO:root:updated the changed fields\n",
      " changed_fields are {'process_queue': {'queue': 'validation'}}\n",
      "INFO:root:\n",
      " got the decision True for the rule id 48\n",
      "INFO:root:\n",
      " updates got are {'process_queue': {'queue': 'validation'}}\n",
      "INFO:root:Host: 35.173.139.208\n",
      "INFO:root:User: root\n",
      "INFO:root:Password: AlgoTeam123\n",
      "INFO:root:Port: 3306\n",
      "INFO:root:Database: deloitte.acelive.ai_extraction\n",
      "DEBUG:root:Making connection to `deloitte.acelive.ai_extraction`...\n",
      "INFO:root:Engine created for `deloitte.acelive.ai_extraction`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Connection established succesfully to `deloitte.acelive.ai_extraction`! (3.96 secs to connect)\n",
      "INFO:root:Host: 35.173.139.208\n",
      "INFO:root:User: root\n",
      "INFO:root:Password: AlgoTeam123\n",
      "INFO:root:Port: 3306\n",
      "INFO:root:Database: deloitte.acelive.ai_queues\n",
      "DEBUG:root:Making connection to `deloitte.acelive.ai_queues`...\n",
      "INFO:root:Engine created for `deloitte.acelive.ai_queues`\n",
      "INFO:root:Connection established succesfully to `deloitte.acelive.ai_queues`! (3.86 secs to connect)\n",
      "INFO:root:Updating table: `process_queue`\n",
      "INFO:root:Update data: `{'queue': 'validation'}`\n",
      "INFO:root:Where clause data: `{'case_id': 'DEL7B6DCE0'}`\n",
      "INFO:root:Force update flag: `False`\n",
      "DEBUG:root:Query: UPDATE `process_queue` SET `queue`=%s WHERE case_id=%s\n",
      "WARNING:root:Query does not have any value to return.\n",
      "INFO:root:\n",
      " params data used from the rules are \n",
      " {'input': [{'type': 'from_table', 'table': 'ocr', 'column': 'total_value', 'value': '1,260,594.00'}]}\n",
      "\n",
      "INFO:root:\n",
      " next rule id to execute is END\n",
      "\n",
      "DEBUG:root:Query: INSERT INTO `rule_data`(`id`, `case_id`, `rule_params`) VALUES ('NULL',%s,%s) ON DUPLICATE KEY UPDATE `rule_params`=%s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'from_table', 'table': 'ocr', 'column': 'total_value', 'value': '1,260,594.00'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Query does not have any value to return.\n",
      "DEBUG:root:Query: INSERT INTO `rule_data` (`id`, `case_id`, `trace_data`) VALUES ('NULL',%s,%s) ON DUPLICATE KEY UPDATE `trace_data`=%s\n",
      "WARNING:root:Query does not have any value to return.\n",
      "INFO:root:\n",
      " Applied chained rules successfully\n",
      "INFO:root:Host: 35.173.139.208\n",
      "INFO:root:User: root\n",
      "INFO:root:Password: AlgoTeam123\n",
      "INFO:root:Port: 3306\n",
      "INFO:root:Database: deloitte.acelive.ai_extraction\n",
      "DEBUG:root:Making connection to `deloitte.acelive.ai_extraction`...\n",
      "INFO:root:Engine created for `deloitte.acelive.ai_extraction`\n",
      "INFO:root:Connection established succesfully to `deloitte.acelive.ai_extraction`! (9.58 secs to connect)\n",
      "INFO:root:Host: 35.173.139.208\n",
      "INFO:root:User: root\n",
      "INFO:root:Password: AlgoTeam123\n",
      "INFO:root:Port: 3306\n",
      "INFO:root:Database: deloitte.acelive.ai_queues\n",
      "DEBUG:root:Making connection to `deloitte.acelive.ai_queues`...\n",
      "INFO:root:Engine created for `deloitte.acelive.ai_queues`\n",
      "INFO:root:Connection established succesfully to `deloitte.acelive.ai_queues`! (3.78 secs to connect)\n"
     ]
    }
   ],
   "source": [
    "case_id = 'DEL7B6DCE0'\n",
    "function_params = {'stage':['extract']}\n",
    "tenant_id = 'deloitte.acelive.ai'\n",
    "r = apply_business_rule(case_id, function_params, tenant_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flag': True,\n",
       " 'message': 'Applied business rules successfully.',\n",
       " 'updates': {}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"rule_type\": \"static\", \"function\": \"Replace\", \"parameters\": {\"data\": {\"source\": \"input_config\", \"table\": \"ocr\", \"column\": \"total_value\"}, \"to_replace\": {\"source\": \"input\", \"value\": \",\"}, \"replace_with\": {\"source\": \"input\", \"value\": \"\"}}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "mount_replace = {  'rule_type': 'static',\n",
    "                        'function': 'Replace',\n",
    "                        'parameters':{\n",
    "                        'data':{'source':'input_config','table':'ocr', 'column':'total_value'},\n",
    "                        'to_replace':{'source':'input','value':','},\n",
    "                        'replace_with':{'source':'input','value':''}\n",
    "    }\n",
    "}\n",
    "\n",
    "x = json.dumps(mount_replace)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"1,23,\"\n",
    "b = \"1,00,000\"\n",
    "operator = \">=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "a = a.replace(',','')\n",
    "b = b.replace(',','')\n",
    "if operator == \">=\":\n",
    "    print(float(a) >= float(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_method\n",
    "def AmountCompare(self,parameters):\n",
    "    left_param, operator, right_param = parameters['left_param'], parameters['operator'], parameters['right_param'] \n",
    "    left_param_value, right_param_value = self.get_param_value(left_param), self.get_param_value(right_param)\n",
    "    logging.debug(f\"left param value is {left_param_value} and type is {type(left_param_value)}\")\n",
    "    logging.debug(f\"right param value is {right_param_value} and type is {type(right_param_value)}\")\n",
    "    logging.debug(f\"operator is {operator}\")\n",
    "    try:\n",
    "        left_param_value = left_param_value.replace(',','')\n",
    "        right_param_value = right_param_value.replace(',','')\n",
    "        if operator == \">=\":\n",
    "            print(float(left_param_value) >= float(right_param_value))\n",
    "            return (float(left_param_value) >= float(right_param_value))\n",
    "    except Exception as e:\n",
    "        logging.debug(f\"error in compare key value {left_param_value} {operator} {right_param_value}\")\n",
    "        logging.debug(str(e))\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
